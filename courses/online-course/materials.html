<html>

<head>
  <title>CS388/AI388/DSC395T</title>
  <link rel="stylesheet" type="text/css" href="courses.css">
</head>

<body>

    <div id="wrapper">
<h1 style="text-align: center;">CS388/AI388/DSC395T: Natural Language Processing (online MS)</h1>

<p>These are the course materials for an online masters course in NLP. All lectures are videos available on YouTube. <b>If you are enrolled in on-campus CS388 or CS371N, this is not the correct website for your course.</b></p>

<p><b>Note on enrollment for on-campus students:</b> These courses are listed in the course catalog as "Natural Language Processing-WB". They are partially asynchronous
courses taught for certain online masters programs at UT ("Option III" programs, as the university calls them). If you are a student enrolled on-campus at UT Austin,
you are <b>not</b> eligible to take this course. This is a hard requirement from
the university due to the fact that this course is part of an Option III program. There is an on-campus version of CS388 that is typically
taught once per year which you are eligible to take (or CS371N if you're an undergraduate student). Regardless, you are free to consult the materials here!

<h2>Assignments</h2>

<p><b><a href="a1.pdf">Assignment 1: Linear Sentiment Classification</a></b> <a href="a1-distrib.tgz">[code and dataset download]</a> [see Canvas for code walkthrough and debugging tips]
<p><b><a href="a2.pdf">Assignment 2: Feedforward Neural Networks, Word Embeddings, and Generalization</a></b> <a href="a2-distrib.tgz">[code and dataset download]</a> [see Canvas for code walkthrough and debugging tips]
<p><b><a href="a3.pdf">Assignment 3: Transformer Language Modeling</a></b> <a href="a3-distrib.tgz">[code and dataset download]</a> [see Canvas for code walkthrough and debugging tips]
<p><b><a href="a4.pdf">[NEW 2024 VERSION] Assignment 4: Factuality and ChatGPT</a></b> <a href="a4-distrib.tgz">[code and dataset download]</a>
<p><b><a href="fp.pdf">Final Project: Dataset Artifacts</a></b> <a href="https://github.com/gregdurrett/fp-dataset-artifacts">[code and dataset download]</a> <a href="project-ex-1.pdf">[example 1]</a> <a href="project-ex-2.pdf">[example 2]</a> <b><a href="peer-assessment.pdf">[peer assessment instructions]</a></b>
<!--<p><b><a href="a6.pdf">[SPRING 2021 VERSION] Assignment 6 (Final Project): Domain Adaptation for Question Answering</a></b> <a href="https://github.com/gregdurrett/nlp-qa-finalproj/">[code and dataset download]</a>-->

<p><b>Midterm</b>: <a href="fa24-midterm-topics.pdf">Midterm topics</a>, <a href="fa23-cs388-midterm.pdf">2023 midterm</a> / <a href="fa23-cs388-midterm-solns.pdf">2023 solutions</a>, <a href="fa22-cs388-midterm.pdf">2022 midterm</a> / <a href="fa22-cs388-midterm-solns.pdf">2022 solutions</a>

<h2>Lecture Videos and Readings</h2>

<p><b><a href="https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7">YouTube playlist containing all videos</a></b></p>

<p><b><a href="./slides-notes.tgz">Download the slides and handwritten notes here (88MB tgz)</a></b></p>

<table border="1" cellpadding="3" cellspacing="0" style="border-collapse: collapse" bordercolor="#111111"
  id="AutoNumber5" width="800">
  <tbody>
    <tr>
      <td width="320" height="18"><b>Topics and Videos</b></td>
      <td width="500" height="18"><b>Readings</b></td>
    </tr>
    <tr><td colspan="2" style="text-align:center"><b>Week 1: Intro and Linear Classification</b></td></tr>
    <tr>
     <td><a href="https://youtu.be/Mz8-LTednt4">Course Preview</a>
     </td>
      <td></td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/k5p8teUNHX4">Introduction</a>
      </td>
      <td><div style="font-size: small">Note: this introduction video is from an older run of the class and references an outdated schedule. Please refer
          to the new course structure here.
          </div>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/DVxR3AwdxoA">Linear Binary
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          2.0-2.5, 4.2-4.4.1</a><br><br>
        <a href="./perc-lr-connections.pdf">Perceptron and logistic regression</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0jSElGFUxro">
          Sentiment Analysis and Basic Feature Extraction</a></td>
      <td><a
          href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          4.1</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/_We4tlPkaj0">Basics of
          Learning, Gradient Descent</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tMGv5ZcuVP4">Perceptron</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hhTkyP7EzGw">Perceptron as
          Minimizing Loss</a>
      </td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0naHFT07ja8">Logistic
          Regression</a>
      </td>
      <td><a href="./perc-lr-connections.pdf">Perceptron and LR connections</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/cKbnEmjxnOY">Sentiment
          Analysis</a>
      </td>
      <td><a href="https://www.aclweb.org/anthology/W02-1011/" target="_blank">Thumbs up? Sentiment Classification using
          Machine Learning Techniques</a> Bo Pang et al., 2002<br></br>
        <a href="https://www.aclweb.org/anthology/P12-2018/" target="_blank">Baselines and Bigrams: Simple, Good
          Sentiment and Topic Classification</a> Sida Wang and Christopher Manning, 2012<br></br>
        <a href="https://www.aclweb.org/anthology/D14-1181/" target="_blank">Convolutional Neural Networks for Sentence
          Classification</a> Yoon Kim, 2014<br></br>
        <a href="https://github.com/sebastianruder/NLP-progress/blob/master/english/sentiment_analysis.md"
          target="_blank">[GitHub] NLP Progress on Sentiment Analysis</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/65ui-GdtY0Q">Optimization
          Basics</a>
      </td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2"><td colspan="2" style="text-align:center"><b>Week 2: Multiclass and Neural Classification</b></td></tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/My6GaGhqxdI">Multiclass
          Classification</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 4.2</a><br><br>
        <a href="./multiclass.pdf">Multiclass lecture note</a>
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/EA627DC7k6M">Multiclass
          Perceptron and Logistic Regression</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/va2i7LXt9zI">Multiclass
          Classification Examples</a></td>
      <td><a href="https://www.aclweb.org/anthology/D15-1075/" target="_blank">A large annotated corpus for learning
          natural language inference</a> Sam Bowman et al., 2015<br></br>
        <a href="https://www.aclweb.org/anthology/D13-1193/" target="_blank">Authorship Attribution of
          Micro-Messages</a> Roy Schwartz et al., 2013
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/N4f2-S19LME">Fairness in
          Classification</a></td>
      <td><a href="https://arxiv.org/pdf/1811.10104.pdf" target="_blank">50 Years of Test (Un)fairness: Lessons for
          Machine Learning</a> Ben Hutchinson and Margaret Mitchell, 2018<br><br>
        <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G"
          target="_blank">[Article] Amazon scraps secret AI recruiting tool that showed bias against women</a>
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/DU_p-RBy5gM">Neural
          Networks</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/rdohzaGa8aE">Neural Network
          Visualization</a></td>
      <td><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">[Blog] Neural Networks,
          Manifolds, and Topology</a> Chris Olah</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/8WhPYIWyR5g">Feedforward
          Neural Networks, Backpropagation</a></td>
      <td><a
          href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          Chapter 3.1-3.3</a></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/IRZCQO18QAI">Neural Net
          Implementation</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/KPZb2rYS4BE">Neural Net
          Training, Optimization</a></td>
      <td><a href="https://dl.acm.org/doi/10.5555/2627435.2670313">Dropout: a simple way to prevent neural networks from
          overfitting</a> Nitish Srivastava et al., 2014 <br></br>

        <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
          Internal Covariate Shift</a> Sergey Ioffe and Christian Szegedy, 2015<br></br>

        <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> Durk Kingma and Jimmy Ba,
        2015<br></br>

        <a href="https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html">The Marginal
          Value of Adaptive Gradient Methods in Machine Learning</a> Ashia Wilson et al., 2017
      </td>
    </tr>

    <tr><td colspan="2" style="text-align:center"><b>Week 3: Word Embeddings</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/8EqQROdVPyM">Word
          Embeddings</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          14.5</a></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/hznxqCIrzSQ">Skip-gram</a>
      </td>
      <td><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed
          Representations of Words and Phrases and their Compositionality</a> Tomas Mikolov et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/gpP-depOUwg">Other Word
          Embedding Methods</a></td>
      <td><a href="https://papers.nips.cc/paper/2008/hash/1e056d2b0ebd5c878c550da6ac5d3724-Abstract.html"
          target="_blank">A Scalable Hierarchical Distributed Language Model</a> Andriy Mnih and Geoff Hinton, 2008<br></br>
        <a href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf"
          target="_blank">Neural Word Embedding as Implicit Matrix Factorization</a> Omer Levy and Yoav Goldberg, 2014<br></br>
        <a href="https://www.aclweb.org/anthology/D14-1162/" target="_blank">GloVe: Global Vectors for Word
          Representation</a> Jeffrey Pennington et al., 2014<br></br>
        <a href="https://arxiv.org/abs/1607.04606" target="_blank">Enriching Word Vectors with
          Subword Information</a> Piotr Bojanowski et al., 2016
    </tr>

    <tr>
      <td><a href="https://youtu.be/J_227g77Jqg">Bias in Word
          Embeddings</a></td>
      <td><a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf"
          target="_blank">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
          Embeddings</a> Tolga Bolukbasi et al., 2016<br></br>
        <a href="https://www.aclweb.org/anthology/N19-1062/" target="_blank">Black is to Criminal as Caucasian is to
          Police: Detecting and Removing Multiclass Bias in Word Embeddings</a> Thomas Manzini et al., 2019<br></br>
        <a href="https://www.aclweb.org/anthology/N19-1061/" target="_blank">Lipstick on a Pig: Debiasing Methods Cover
          up Systematic Gender Biases in Word Embeddings But do not Remove Them</a> Hila Gonen and Yoav Goldberg, 2019
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/3pwwdHuH0I4">Applying
          Embeddings, Deep Averaging Networks</a></td>
      <td><a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals
          Syntactic Methods for Text Classification</a> Mohit Iyyer et al., 2015</td>
    </tr>


    <tr bgcolor="#F2F2F2"><td colspan="2" style="text-align:center"><b>Week 4: Language Modeling and Self-Attention</b></td></tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/J-yHbD8LYCM">
          n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.1</a></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/Yfug5eIQh5w">
          Smoothing in n-gram LMs</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.2</a></td>
    </tr>
    
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/ImW4vJ5XZQc">
          LM Evaluation</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.4</a></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/59NrmwAdOWA">
          Neural Language Models</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/xvnnA04JVQo">
          RNNs and their Shortcomings</a></td>
      <td><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein
          6.3</a><br><br/>
<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">[Blog] Understanding LSTMs</a> Chris Olah</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/q7HY7tpWWi8">
          Attention</a></td>
      <td><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate
</a> Dzmitry Bahdanau et al., 2015</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/10l2NXStROU">
          Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/nHXrdLMo8Uk">
          Multi-Head Self-Attention</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<br><br>
        <a href="http://jalammar.github.io/illustrated-transformer/">[Blog] The Illustrated Transformer</a> Jay Alammar</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/a8sTGth7PoU">
          Position Encodings</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017<br><br>
        <a href="https://arxiv.org/abs/2108.12409" target="_blank">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a> Ofir Press et al., 2021<br><br>
        <a href="https://arxiv.org/abs/2305.19466" target="_blank">The Impact of Positional Encoding on Length Generalization in Transformers</a> Amirhossein Kazemnejad et al., 2023
      </td>
    </tr>

    <tr><td colspan="2" style="text-align:center"><b>Week 5: Transformers and Decoding</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/sLsUD-RcDqg">
          Transformer Architecture</a></td>
      <td><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Attention Is All You Need</a> Ashish Vaswani et al.,
        2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/1Efx04lHa7w">
          Using Transformers</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/htyspM3FrMg">
          Transformer Language Modeling</a></td>
      <td>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/DPvDL8L4Dqo">
          Transformer Extensions</a></td>
      <td>
        <a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a> Jared Kaplan et al., 2020<br><br>
        <a href="https://arxiv.org/abs/2009.06732" target="_blank">Efficient Transformers: A Survey</a> Yi Tay et al., 2020<br><br>
        <a href="https://arxiv.org/abs/2009.14794" target="_blank">Rethinking Attention with Performers</a> Krzysztof Choromanski et al., 2021<br><br>
        <a href="https://arxiv.org/abs/2004.05150" target="_blank">Longformer: The Long-Document Transformer</a> Iz Beltagy et al., 2021

      </td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/wltqDbhlcJ0">
          Beam Search</a></td>
      <td>
      </td>
    </tr>


    <tr>
      <td><a href="https://youtu.be/JETxaSaj6_k">
          Nucleus Sampling</a></td>
      <td>
         <a href="https://arxiv.org/abs/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a> Ari Holtzman et al., 2019
      </td>
    </tr>


    <tr bgcolor="#F2F2F2"><td colspan="2" style="text-align:center"><b>Week 6: Pre-training, seq2seq LMs</b></td></tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/dya_QNFvtiQ">
          BERT: Masked Language Modeling</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/g96oi4ihc_E">
          BERT: Model and Applications</a></td>
      <td><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">BERT: Pre-training of Deep Bidirectional
          Transformers for Language Understanding</a> Jacob Devlin et al., 2019<br></br>
        <a href="https://www.aclweb.org/anthology/W19-4302/" target="_blank">To Tune or Not to Tune? Adapting Pretrained
          Representations to Diverse Tasks</a> Matthew Peters et al., 2019<br></br>
        <a href="https://arxiv.org/pdf/1804.07461.pdf" target="_blank">GLUE: A Multi-Task Benchmark and Analysis
          Platform for Natural Language Understanding</a> Alex Wang et al., 2019<br></br>
        <a href="https://arxiv.org/abs/1906.04341" target="_blank">What Does BERT Look At? An Analysis of BERT's Attention
           </a> Kevin Clark et al., 2019
        <a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank">RoBERTa: A Robustly Optimized BERT Pretraining
          Approach</a> Yinhan Liu et al., 2019
      </td>
    </tr>


    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/TKZkvqb-qpM">
          Seq2seq Models</a></td>
      <td>
      </td>
    </tr>
    
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/M9L3gk4ITec">
          BART</a></td>
      <td><a href="https://arxiv.org/abs/1910.13461" target="_blank">BART: Denoising Sequence-to-Sequence Pre-training
          for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et al., 2019
      </td>
    </tr>
    
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/b6KFaT8mK4g">
          T5</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">Exploring the Limits of Transfer Learning with a
          Unified Text-to-Text Transformer</a> Colin Raffel et al., 2020<br><br>
        <a href="https://arxiv.org/abs/2005.00700" target="_blank">UnifiedQA: Crossing Format Boundaries With a Single QA System</a> Daniel Khashabi et al., 2020<br><br>
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/WA16JelEkkg">
          Word Piece and Byte Pair Encoding</a></td>
      <td><a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank">Neural Machine Translation of Rare Words with
          Subword Units</a> Rico Sennrich et al., 2016<br><br>
        <a href="https://arxiv.org/pdf/2004.03720.pdf" target="_blank">Byte Pair Encoding is Suboptimal for Language
          Model Pretraining</a> Kaj Bostrom and Greg Durrett, 2020
      </td>
    </tr>
    <tr><td colspan="2" style="text-align:center"><b>Week 7-8: Structured Prediction: Part-of-speech, Syntactic Parsing</b><br>
      <div style="font-size: small">Note: this unit was previously presented as Week 4 right after classification. There are a few references
          to it being our first brush with structured models. In this structure of the course, it's still true that it's our first exposure to
          models dealing with linguistic structure as opposed to surface-level sequential structure (i.e., token sequences in generation).</div>
      </td></tr>
    <tr>
      <td><a href="https://youtu.be/Llw6qfeAWDs">Part-of-Speech
          Tagging</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 8.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/yQZ0mDW-U3g">Sequence
          Labeling, Tagging with Classifiers</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/FeLtLLbn4qU">Hidden Markov
          Models</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dVF7LZkbl9g">
          HMMs: Parameter Estimation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.4.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Ks7IrsjhqSo">
          HMMs: Viterbi Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 7.3</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/wijpAX_LLXo">
          HMMs for POS Tagging</a></td>
      <td><a href="https://arxiv.org/abs/cs/0003055" target="_blank">TnT - A Statistical Part-of-Speech Tagger</a>
        Thorsten Brants, 2000<br></br>
        <a href="https://www.aclweb.org/anthology/W00-1308/" target="_blank">Enriching the Knowledge Sources Used in a
          Maximum Entropy Part-of-Speech Tagger</a> Kristina Toutanvoa and Christopher Manning, 2000<br></br>
        <a href="https://link.springer.com/chapter/10.1007/978-3-642-19400-9_14" target="_blank">Part-of-Speech Tagging
          from 97% to 100%: Is It Time for Some Linguistics?</a> Christopher Manning, 2011<br></br>
        <a href="https://www.aclweb.org/anthology/D17-1309.pdf" target="_blank">Natural Language Processing with Small
          Feed-Forward Networks</a> Jan Botha et al., 2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/zDPUKQKDaMM">
          Constituency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.1-10.2</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/q3dLP9YQLPA">
          Probabilistic Context-Free Grammars</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3-10.4</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QeDb6mSDSqs">
          CKY Algorithm</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.3.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/f1o1_bPWzM0">
          Refining Grammars</a></td>
      <td><a href="https://www.aclweb.org/anthology/P03-1054/" target="_blank">Accurate Unlexicalized Parsing</a> Dan Klein
        and Chris Manning, 2003<br><br>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 10.5</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dbDjKCc4R3E">
          Dependencies</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.1</a><br><br>
<a href="https://www.aclweb.org/anthology/Q13-1002/" target="_blank">Finding Optimal 1-Endpoint-Crossing
          Trees</a> Emily Pitler et al., 2013</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ypoaw7lJ6Rk">
          Transition-based Dependency Parsing</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 11.3</a>
      </td>
    </tr>
    <tr bgcolor="#F2F2F2"><td colspan="2" style="text-align:center"><b>Week 9: Modern Large Language Models</b></td></tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/jn41DLgnqek">
          GPT-3</a></td>
      <td>
       <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> Alec Radford et al., 2019<br><br>
       <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> Tom B. Brown et al., 2020<br><br>
       <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a> Hugo Touvron et al., 2023<br><br>
        <div style="font-size: small">Llama 2 is one of the latest models with publicly available weights (although it is not fully open-source, as many details of the training are not public).</div>
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/YCq6b31Jb6E">
          Zero-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2212.04037">Demystifying Prompts in Language Models via Perplexity Estimation</a> Hila Gonen et al., 2022
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/JSBjj09xJeM">
          Few-shot Prompting</a></td>
      <td>
       <a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> Tony Z. Zhao et al., 2021<br><br>
       <a href="https://arxiv.org/abs/2211.09110">Holistic Evaluation of Language Models</a> Percy Liang et al., 2022<br><br>
       <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a> Sewon Min et al., 2022
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/mUthsZ_Aivo">
          Understanding ICL: Induction Heads</a></td>
      <td>
        <a href="https://arxiv.org/abs/2209.11895">In-context Learning and Induction Heads</a> Catherine Olsson et al., 2022
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/YT3VSlDjrVU">
          Instruction Tuning</a></td>
      <td>
        <a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a> Victor Sanh et al., 2021<br><br>
        <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a> Hyung Won Chung et al., 2022
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/DwAdhx6GFh8">
          Reinforcement Learning from Human Feedback (RLHF)</a></td>
      <td>
        <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> Long Ouyang et al., 2022<br><br>
        <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">[Website] Stanford Alpaca: An Instruction-following LLaMA Model</a> Rohan Taori et al., 2023
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/bQZvmQUlqcs">
          Factuality of LLMs</a></td>
      <td>
        <a href="https://arxiv.org/abs/2212.07981">Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation</a> Yixin Liu et al., 2023<br><br>
        <a href="https://arxiv.org/abs/2303.01432">WiCE: Real-World Entailment for Claims in Wikipedia</a> Ryo Kamoi et al., 2023<br><br>
        <a href="https://arxiv.org/abs/2111.09525">SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization</a> Philippe Laban et al., 2022<br><br>
        <a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a> Sewon Min et al., 2023<br><br>
        <a href="https://arxiv.org/abs/2210.08726">RARR: Researching and Revising What Language Models Say, Using Language Models</a> Luyu Gao et al., 2022
      </td>
    </tr>
    <tr><td colspan="2" style="text-align:center"><b>Week 10: Explanations</b></td></tr>


    <tr>
      <td><a href="https://youtu.be/Nr0_xYEso-4">
          Explainability in NLP</a></td>
      <td><a href="https://arxiv.org/pdf/1606.03490.pdf" target="_blank">The Mythos of Model Interpretability</a> Zach Lipton,
        2016<br></br>
        <a href="https://www.aclweb.org/anthology/P15-1162/" target="_blank">Deep Unordered Composition Rivals Syntactic
          Methods for Text Classification</a> Mohit Iyyer et al., 2015<br></br>
        <a href="https://arxiv.org/pdf/1812.08951.pdf" target="_blank">Analysis Methods in Neural Language Processing: A
          Survey</a> Yonatan Belinkov and Jim Glass, 2019
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/ZVElc4CvHpk">
          Local Explanations: Highlights</a></td>
      <td><a href="https://arxiv.org/pdf/1602.04938.pdf" target="_blank">"Why Should I Trust You?" Explaining the
          Predictions of Any Classifier</a> Marco Tulio Ribeiro et al., 2016<br></br>
        <a href="https://arxiv.org/pdf/1703.01365.pdf" target="_blank">Axiomatic Attribution for Deep Networks</a>
        Mukund Sundararajan et al., 2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/a6u6WM5wcLQ">
          Model Probing</a></td>
      <td><a href="https://arxiv.org/pdf/1905.05950.pdf" target="_blank">BERT Rediscovers the Classical NLP Pipeline</a>
        Ian Tenney et al., 2019<br></br>
        <a href="https://arxiv.org/pdf/1905.06316.pdf" target="_blank">What Do You Learn From Context? Probing For
          Sentence Structure In Contextualized Word Represenations</a> Ian Tenney et al., 2019
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/RXYaMZcDIWU">
          Annotation Artifacts</a></td>
      <td><a href="https://www.aclweb.org/anthology/N18-2017/" target="_blank">Annotation Artifacts in Natural Language
          Inference Data</a> Suchin Gururangan et al., 2018<br></br>
        <a href="https://www.aclweb.org/anthology/S18-2023/" target="_blank">Hypothesis Only Baselines in Natural
          Language Inference</a> Adam Poliak et al., 2018<br></br>
        <a href="https://www.aclweb.org/anthology/P18-1176/" target="_blank">Did the Model Understand the Question?</a>
        Pramod Kaushik Mudrakarta et al., 2018<br></br>
        <a href="https://www.aclweb.org/anthology/D18-1009.pdf" target="_blank">Swag: A Large-Scale Adversarial Dataset
          for Grounded Commonsense Inference</a> Rowan Zellers et al., 2018
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/bXHM5t_ejsc">
          Text Explanations</a></td>
      <td>
        <a href="https://arxiv.org/pdf/1603.08507.pdf" target="_blank">Generating Visual Explanations</a> Lisa-Anne Hendricks et
        al., 2016<br></br>
        <a href="https://arxiv.org/abs/1812.01193" target="_blank">e-SNLI: Natural Language Inference with Natural Language Explanations</a> Oana-Maria Camburu et
        al., 2018<br></br>
        <a href="https://arxiv.org/pdf/2004.05569.pdf" target="_blank">Explaining Question Answering Models through Text
          Generation</a> Veronica Latcinnik and Jonathan Berant, 2020
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/tNGu3EqJbKc">
          Chain-of-thought</a></td>
      <td>
        <a href="https://arxiv.org/abs/1705.04146" target="_blank">Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems</a> Wang Ling et al., 2017<br><br>
        <a href="https://arxiv.org/abs/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> Jason Wei et al., 2022<br><br>
        <a href="https://arxiv.org/abs/2205.03401" target="_blank">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning</a> Xi Ye and Greg Durrett, 2022<br><br>
        <a href="https://arxiv.org/abs/2205.11916" target="_blank">Large Language Models are Zero-Shot Reasoners</a> Takeshi Kojima et al., 2022
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/9sFyzMywKmo">
          Chain-of-thought: Extensions and Analysis</a></td>
      <td>
        <a href="https://arxiv.org/pdf/2211.13892.pdf" target="_blank">Complementary Explanations for Effective In-Context Learning</a> Xi Ye et al., 2023<br><br>
        <a href="https://arxiv.org/abs/2211.10435" target="_blank">PAL: Program-aided Language Models</a> Luyu Gao et al., 2022<br><br>
        <a href="https://arxiv.org/abs/2210.03350" target="_blank">Measuring and Narrowing the Compositionality Gap in Language Models</a> Ofir Press et al., 2022
      </td>
    </tr>

    <tr><td colspan="2" style="text-align:center"><b>Week 11: Question Answering, Dialogue Systems</b></td></tr>
    
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/gnUSE0fCbso">
          Reading comprehension intro</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/JRI3RwRBnMY">
          Reading comprehension: setup and baselines</a></td>
      <td><a href="https://www.aclweb.org/anthology/D13-1020.pdf" target="_blank">MCTest: A Challenge Dataset for the
          Open-Domain Machine Comprehension of Text</a> Matthew Richardson et al., 2013<br></br>
        <a href="https://www.aclweb.org/anthology/D16-1264/" target="_blank">SQuAD: 100,000+ Questions for Machine
          Comprehension of Text</a> Pranav Rajpurkar et al., 2016
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/F8hWZ4xaVkA">
          BERT for QA</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/tCvAHmrxPvY">
          Problems with Reading Comprehension</a></td>
      <td><a href="https://www.aclweb.org/anthology/D17-1215/" target="_blank">Adversarial Examples for Evaluating
          Reading Comprehension Systems</a> Robin Jia and Percy Liang, 2017</td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/P-j_zeS0Pa8">
          Open-domain QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1704.00051" target="_blank">Reading Wikipedia to Answer Open-Domain Questions</a> Danqi Chen et al., 2017<br><br>
        <a href="https://www.aclweb.org/anthology/P19-1612.pdf" target="_blank">Latent Retrieval for Weakly Supervised
          Open Domain Question Answering</a> Kenton Lee et al., 2019<br><br>
        <a href="https://ai.google.com/research/NaturalQuestions" target="_blank">[Website] Natural Questions</a> Tom Kwiatkowski et al., 2019<br><br>
        <div style="font-size: small">Most modern open-domain QA systems are either "closed-book" models like ChatGPT or "open-book" models that do retrieval, similar
          to the Chen et al. and Lee et al. papers above. These are typically described under the general framework of <a href="https://arxiv.org/pdf/2005.11401.pdf">retrieval-augmented generation</a> and an example of
          how these systems work is <a href="https://arxiv.org/abs/2112.09332">WebGPT</a> (similar to the "new Bing" chatbot).</div>
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/jpRwa2iE_z8">
          Multi-hop QA</a></td>
      <td>
        <a href="https://arxiv.org/abs/1809.09600" target="_blank">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
             </a> Zhilin Yang et al., 2018<br></br>
        <a href="https://www.aclweb.org/anthology/N19-1405/" target="_blank">Understanding Dataset Design Choices for
          Multi-hop Reasoning</a> Jifan Chen and Greg Durrett, 2019<br></br>
        <a href="https://openreview.net/forum?id=SJgVHkrYDH" target="_blank">Learning to Retrieve Reasoning Paths over
          Wikipedia Graph for Question Answering</a> Akari Asai et al., 2020<br><br>
        <div style="font-size: small">Modern QA systems operating over the web are largely multi-hop by default; multi-hop QA has been subsumed by open-domain QA to a large extent. For a more recent multi-hop QA dataset, see <a href="https://arxiv.org/abs/2205.12665">QAMPARI</a></div>

      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/vAZ7VlLXReE">
          Dialogue: Chatbots</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/JXfAkX7kvnM">
          Task-Oriented Dialogue</a></td>
      <td><a href="https://arxiv.org/pdf/1811.01241.pdf" target="_blank">Wizards of Wikipedia: Knowledge-Powered
          Conversational Agents</a> Emily Dinan et al., 2019<br><br>
          <a href="https://arxiv.org/abs/2009.11423" target="_blank">Task-Oriented Dialogue as Dataflow Synthesis</a> Semantic Machines, 2020</td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/Hc7P3QukmJk" target="_blank">
          Neural Chatbots</a></td>
      <td>
        <a href="https://arxiv.org/abs/1506.06714">A Neural Network Approach to Context-Sensitive Generation of Conversational Responses</a> Alessandro Sordoni et al., 2015<br><br>
        <a href="https://arxiv.org/abs/1510.03055">A Diversity-Promoting Objective Function for Neural Conversation Models</a> Jiwei Li et al., 2016<br><br>
        <a href="https://arxiv.org/pdf/2004.13637.pdf">Recipes for building an open-domain chatbot</a> Stephen Roller et al., 2020<br><br>
        <div style="font-size: small">Note: an updated version of BlenderBot is described in <a href="https://arxiv.org/abs/2208.03188">Kurt Shuster et al.</a>.
              Other chatbots discussed, like <a href="https://character.ai">character.ai</a>, can be found online and you can play with them, but less information
              about their precise internals is available in published papers.</div>
      </td>
          
    </tr>

    <tr><td colspan="2" style="text-align:center"><b>Week 12: Machine Translation, Summarization</b></td></tr>
    <tr>
      <td><a href="https://youtu.be/9KAZ4-gKj9g">
          Machine Translation Intro</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/Oup0DEYJXEQ">
          MT: Framework and Evaluation</a></td>
      <td>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.1</a>
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/dzOuPhBmFtE">
          MT: Word alignment</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/mbtk3VCG_2A">
          MT: IBM Models</a></td>
      <td><a href="https://www.aclweb.org/anthology/C96-2141.pdf" target="_blank">HMM-Based Word Alignment in
          Statistical Translation</a> Stephan Vogel et al., 1996</td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/0k8b5jGk-h4">
          Phrase-based Machine Translation</a></td>
      <td><a href="http://homepages.inf.ed.ac.uk/pkoehn/publications/pharaoh-amta2004.pdf" target="_blank">Pharaoh: A
          Beam Search Decoder for Phrase-Based Statistical Machine Translation Models</a> Philipp Koehn, 2004<br><br>
        <a href="https://www.aclweb.org/anthology/P03-1021/" target="_blank">Minimum Error Rate Training in Statistical
          Machine Translation</a> Franz Och, 2003<br><br>
        <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Eisenstein 18.4</a>
      </td>
    </tr>
    <tr>
      <td><a href="https://youtu.be/bcP4b_4HQ8A">
          Neural and Pre-Trained Machine Translation</a></td>
      <td>
        <a href="https://arxiv.org/abs/1905.11901" target="_blank">Revisiting Low-Resource Neural Machine Translation: A Case Study</a> Rico Sennrich and Biao Zhang, 2019<br><br>
        <a href="https://aclanthology.org/2020.acl-main.688/" target="_blank">In Neural Machine Translation, What Does Transfer Learning Transfer?</a> Alham Fikri Aji et al., 2020<br><br>
        <a href="https://arxiv.org/abs/2001.08210" target="_blank">Multilingual Denoising Pre-training for Neural Machine Translation</a> Yinhan Liu et al., 2020<br><br>
        <a href="https://arxiv.org/abs/2302.14520" target="_blank">Large Language Models Are State-of-the-Art Evaluators of Translation Quality</a> Tom Kocmi and Christian Federmann, 2023
      </td>
    </tr>
    
    <tr>
      <td><a href="https://youtu.be/lBDI1CBNe_U">
          Summarization Intro</a></td>
      <td></td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/QWt2E3m00kA">
          Extractive Summarization</a></td>
      <td><a href="https://dl.acm.org/doi/10.1145/290941.291025" target="_blank">The use of MMR, diversity-based
          reranking for reordering documents and producing summaries</a> Jaime Carbonell and Jade Goldstein, 1998<br></br>
        <a href="https://arxiv.org/abs/1109.2128" target="_blank">LexRank: Graph-based Lexical
          Centrality as Salience in Text Summarization</a> Gunes Erkan and Dragomir Radev, 2004<br></br>
        <a href="https://www.aclweb.org/anthology/W09-1802/" target="_blank">A Scalable Global Model for
          Summarization</a> Dan Gillick and Benoit Favre, 2009<br></br>
        <a href="https://www.aclweb.org/anthology/W17-4511/" target="_blank">Revisiting the Centroid-based Method: A
          Strong Baseline for Multi-Document Summarization</a> Demian Gholipour Ghalandari, 2017
      </td>
    </tr>

    <tr>
      <td><a href="https://youtu.be/feLTtTilycY">
          Pre-trained Summarization and Factuality</a></td>
      <td><a href="https://www.aclweb.org/anthology/2020.acl-main.703/" target="_blank">BART: Denoising
          Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> Mike Lewis et
        al., 2019<br></br>
        <a href="https://arxiv.org/abs/1912.08777" target="_blank">PEGASUS:
          Pre-training with Extracted Gap-sentences for Abstractive Summarization</a> Jingqing Zhang et al., 2020<br></br>
        <a href="https://arxiv.org/pdf/2010.05478.pdf" target="_blank">Evaluating Factuality in Generation with
          Dependency-level Entailment</a> Tanya Goyal and Greg Durrett, 2020<br><br>
        <a href="https://arxiv.org/abs/2004.04228" target="_blank">Asking and Answering Questions to Evaluate the Factual Consistency of Summaries</a> Alex Wang et al., 2020<br><br>
        <div style="font-size: small">Note: while the specific fine-tuned modeling approaches and factuality detection systems are no longer state-of-the-art as stated in the video,
              they are representative of ideas from pre-training 
              that are still used today. For discussion of how LLMs relate to summarization, see <a href="https://arxiv.org/abs/2209.12356">News Summarization and Evaluation in the Era of GPT-3
</a> by Tanya Goyal, Junyi Jessy Li, and Greg Durrett</div>
      </td>
    </tr>

    <tr><td colspan="2" style="text-align:center"><b>Week 13-14: Multilinguality, Language Grounding, Ethical Issues</b></td></tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/ettP9Ayrho8">
          Morphology</a></td>
      <td></td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/rTSCLfxdxrI">
          Cross-lingual Tagging and Parsing</a></td>
      <td><a href="https://www.aclweb.org/anthology/P11-1061/" target="_blank">Unsupervised Part-of-Speech Tagging with
          Bilingual Graph-Based Projections</a> Dipanjan Das and Slav Petrov, 2011<br><br>
        <a href="https://www.aclweb.org/anthology/D11-1006/" target="_blank">Multi-Source Transfer of Delexicalized
          Dependency Parsers</a> Ryan McDonald et al., 2011
      </td>
    </tr>

    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/KNBRb8sjzOA">
          Cross-lingual Pre-training</a></td>
      <td><a href="https://arxiv.org/pdf/1602.01925.pdf" target="_blank">Massively Multilingual Word Embeddings</a>
        Waleed Ammar et al., 2016<br><br>
        <a href="https://www.aclweb.org/anthology/Q19-1038.pdf" target="_blank">Massively Multilingual Sentence
          Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a> Mikel Artetxe and Holger Schwenk, 2019<br><br>
        <a href="https://www.aclweb.org/anthology/P19-1493.pdf" target="_blank">How multilingual is Multilingual
          BERT?</a> Telmo Pires et al., 2019
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/ayNoMmoXnd8">
          Language Grounding</a></td>
      <td>
        <a href="https://aclanthology.org/2020.acl-main.463/">Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data</a> Emily Bender and Alexander Koller, 2020<br><br>
        <a href="https://arxiv.org/abs/2104.10809">Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?</a> Will Merrill et al., 2021<br><br>
        <a href="https://arxiv.org/abs/2209.12407">Entailment Semantics Can Be Extracted from an Ideal Language Model</a> Will Merrill et al., 2022<br><br>
        <a href="https://arxiv.org/abs/2004.10151">Experience Grounds Language</a> Yonatan Bisk et al., 2020
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://www.youtube.com/watch?v=pkdV-iddZxk">
          Language and Vision</a></td>
      <td>
       <a href="https://arxiv.org/abs/1505.00468" target="_blank">VQA: Visual Question Answering</a> Aishwarya Agrawal et al., 2015<br><br>
       <a href="https://arxiv.org/abs/2103.00020" target="_blank">Learning Transferable Visual Models From Natural Language Supervision</a> Alex Radford et al., 2021
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/tTkAjNkXvH4">
          Ethics: Bias</a></td>
      <td>
        <a href="https://aclanthology.org/P16-2096.pdf" target="_blank">The Social Impact of Natural Language Processing</a> Dirk Hovy and Shannon Spruit, 2016<br><br>
        <a href="https://arxiv.org/pdf/1707.09457.pdf" target="_blank">Men Also Like Shopping:
Reducing Gender Bias Amplification using Corpus-level Constraints</a> Jieyu Zhao et al., 2017
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/0haVbW2ouzw">
          Ethics: Exclusion</a></td>
      <td>
        <a href="https://arxiv.org/abs/2205.12247" target="_blank">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</a> Da Yin et al., 2022<br><br>
        <a href="https://arxiv.org/abs/2109.13238" target="_blank">Visually Grounded Reasoning across Languages and Cultures</a> Fangyu Liu et al., 2021
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/2PSzHb08Xm4">
          Ethics: Dangers of Automation</a></td>
      <td>
       <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a> Emily Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell, 2021<br><br>
       <a href="https://arxiv.org/abs/2009.11462" target="_blank">RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</a> Samuel Gehman et al., 2020
      </td>
    </tr>
    <tr bgcolor="#F2F2F2">
      <td><a href="https://youtu.be/Hp8qfbsp57M">
          Ethics: Unethical Use and Paths Forward</a></td>
      <td>
       <a href="https://arxiv.org/pdf/1803.09010.pdf">Datasheets for Datasets</a> Timnit Gebru et al., 2018<br><br>
       <a href="https://dl.acm.org/doi/pdf/10.1145/3351095.3372873">Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing</a> Deb Raji et al., 2020
      </td>
    </tr>

  </tbody>
</table>

</p>

</div>
</div>
</body>

</html>
